# Big Data Analysis with PySpark

This project consists of four separate tasks analyzing large-scale real-world datasets using Apache Spark. It was developed as part of my coursework project for the Big Data Processing module in the MSc Big Data Science program at Queen Mary University of London.

Each task explores a unique dataset and uses Spark’s distributed processing capabilities to extract insights, perform transformations, and generate visualizations using PySpark, Spark SQL, and Pandas.

---

##  Project Structure

The repository is organized by task:

- `task1_twitter_hashtags/`  
  ➤ Extracted hashtags from tweet data, performed frequency analysis, and visualized trending topics.

- `task2_movie_ratings/`  
  ➤ Analyzed movie ratings data to compute average ratings by genre, user patterns, and rating distributions.

- `task3_taxi_trips/`  
  ➤ Explored Chicago taxi trip data to uncover trends in distance, fare amounts, and trip durations.

- `task4_hdfs_logs/`  
  ➤ Processed large HDFS logs to extract operation-level insights and identify storage access patterns.

---

##  Tools & Technologies

- Apache Spark (PySpark)
- Spark SQL
- Python, Pandas
- Jupyter Notebooks
- Matplotlib, Seaborn (for visualizations)

---

##  Project Highlights

- Worked with large datasets using distributed computing on Spark.
- Applied transformations using RDDs and DataFrames.
- Performed aggregations, joins, filters, and time-based analysis.
- Visualized key insights using Matplotlib and Seaborn.

---

##  Data Access Note

> ⚠️ **Note:**  
The datasets used in this project were stored on a shared Spark cluster (JupyterHub) provided by Queen Mary University of London. Due to access restrictions, these datasets are not included in this repository.  
The notebooks are provided to demonstrate the analysis pipeline and Spark logic.

---

##  How to Use

These notebooks are meant for review and reference. To run them:
1. Set up Apache Spark and Jupyter locally or in a cloud environment.
2. Modify the file paths to point to your local or accessible data.
3. Run the `.ipynb and .py` notebooks inside each task folder.

---



## Sample Output
![image](https://github.com/user-attachments/assets/627be2c7-c970-48ea-a92f-fb1f01f3066e)

![image](https://github.com/user-attachments/assets/b05c9ea1-288e-4285-b231-325326596f9b)




##  Author

**Sarbesh Bhatta**  
MSc Big Data Science — Queen Mary University of London  
[LinkedIn](https://www.linkedin.com/in/sarbeshb7) | 
